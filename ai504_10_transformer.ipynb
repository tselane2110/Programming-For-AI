{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dE_MQj2V-mAD"
   },
   "source": [
    "# [AI 504] Programming for AI, Fall 2021\n",
    "# Practice 10: Transformers\n",
    "----- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okJXS1-OEbN7"
   },
   "source": [
    "#### [Notifications]\n",
    "- If you have any questions, feel free to ask\n",
    "- For additional questions, send emails: yeonsu.k@kaist.ac.kr    \n",
    "      \n",
    "\n",
    "     \n",
    "     \n",
    "# Table of contents\n",
    "1. [Prepare input](#1)\n",
    "2. [Implement Transformer](#2)\n",
    "3. [Train and Evaluate](#3)\n",
    "4. [Visualize attention](#4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2Bj-rF9EbN7"
   },
   "source": [
    "# Prepare essential packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHrI30vIEbN7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "!pip install torchtext==0.10.0\n",
    "!git clone https://github.com/sjpark9503/attentionviz.git\n",
    "!python -m spacy download de\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sayXpp8FEbN8"
   },
   "source": [
    "# I. Prepare input\n",
    "<a id='1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --recursive https://github.com/multi30k/dataset.git multi30k-datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find multi30k-datase/ -name '*.gz' -exec gunzip {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KWOpLYfEbN8"
   },
   "source": [
    "We've already learned how to preprocess the text data in week 8, 9 & 10.\n",
    "\n",
    "You can see some detailed explanation about translation datasets in [torchtext](https://pytorch.org/text/), [practice session,week 9](https://classum.com/main/course/7726/103) and [PyTorch NMT tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaduS25kEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import os\n",
    "import io\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, root_dir, split):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "\n",
    "        self.data_files = {\n",
    "            'train': ('train.de', 'train.en'),\n",
    "            'valid': ('val.de', 'val.en'),\n",
    "            'test': ('test_2016_flickr.de', 'test_2016_flickr.en')\n",
    "        }\n",
    "\n",
    "        self.de_file_path = os.path.join(self.root_dir, self.data_files[self.split][0])\n",
    "        self.en_file_path = os.path.join(self.root_dir, self.data_files[self.split][1])\n",
    "\n",
    "        with io.open(self.de_file_path, mode='r', encoding='utf-8') as de_file, \\\n",
    "             io.open(self.en_file_path, mode='r', encoding='utf-8') as en_file:\n",
    "            self.de_sentences = de_file.readlines()\n",
    "            self.en_sentences = en_file.readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.de_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        de_sentence = tokenize_de(self.de_sentences[idx].strip())\n",
    "        en_sentence = tokenize_en(self.en_sentences[idx].strip())\n",
    "        return {'SRC': de_sentence, 'TRG': en_sentence}\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, counter, min_freq):\n",
    "        self.itos = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "        self.stoi = {token: i for i, token in enumerate(self.itos)}\n",
    "        self.min_freq = min_freq\n",
    "        self.build_vocab(counter)\n",
    "\n",
    "    def build_vocab(self, counter):\n",
    "        for word, freq in counter.items():\n",
    "            if freq >= self.min_freq and word not in self.stoi:\n",
    "                self.stoi[word] = len(self.itos)\n",
    "                self.itos.append(word)\n",
    "\n",
    "    def numericalize(self, tokens):\n",
    "        return [self.stoi.get(token, self.stoi['<unk>']) for token in tokens]\n",
    "\n",
    "def build_counter(dataset):\n",
    "    counter = Counter()\n",
    "    for i in range(len(dataset)):\n",
    "        example = dataset[i]\n",
    "        counter.update(example['SRC'])\n",
    "        counter.update(example['TRG'])\n",
    "    return counter\n",
    "\n",
    "train_data = TranslationDataset(root_dir='/content/multi30k-datase/data/task1/raw', split='train')\n",
    "valid_data = TranslationDataset(root_dir='/content/multi30k-datase/data/task1/raw', split='valid')\n",
    "test_data = TranslationDataset(root_dir='/content/multi30k-datase/data/task1/raw', split='test')\n",
    "\n",
    "counter = build_counter(train_data)\n",
    "SRC_vocab = Vocab(counter, min_freq=2)\n",
    "TRG_vocab = Vocab(counter, min_freq=2)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch = [torch.tensor(SRC_vocab.numericalize(item['SRC'])) for item in batch]\n",
    "    trg_batch = [torch.tensor(TRG_vocab.numericalize(item['TRG'])) for item in batch]\n",
    "\n",
    "    src_batch_padded = pad_sequence(src_batch, padding_value=SRC_vocab.stoi['<pad>'], batch_first=True)\n",
    "    trg_batch_padded = pad_sequence(trg_batch, padding_value=TRG_vocab.stoi['<pad>'], batch_first=True)\n",
    "\n",
    "    return {'SRC': src_batch_padded, 'TRG': trg_batch_padded}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "train_iterator = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "valid_iterator = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_iterator = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fE_u1Qg-EbN8"
   },
   "source": [
    "# II. Implement Transformer\n",
    "<a id='2'></a>\n",
    "In practice week 11, we will learn how to implement the __[Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (Vaswani et al., 2017)__\n",
    "\n",
    "The overall architecutre is as follows:\n",
    "![picture](http://incredible.ai/assets/images/transformer-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqEVVfl-EbN8"
   },
   "source": [
    "## 1. Basic building blocks\n",
    "\n",
    "In this sections, we will implement the building blocks of the transformer: [Multi-head attention](#1a), [Position wise feedforward network](#1b) and [Positional encoding](#1c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeI2oINrEbN8"
   },
   "source": [
    "### a. Attention\n",
    "<a id='1a'></a>\n",
    "In this section, you will implement scaled dot-product attention and multi-head attention.\n",
    "\n",
    "__Scaled dot product:__\n",
    "\n",
    "![picture](http://incredible.ai/assets/images/transformer-scaled-dot-product.png)\n",
    "\n",
    "__Multi-head attention:__\n",
    "\n",
    "![picture](http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)\n",
    "Equation:\n",
    "\n",
    "$$\\begin{align} \\text{MultiHead}(Q, K, V) &= \\text{Concat}(head_1, ...., head_h) W^O \\\\\n",
    "\\text{where head}_i &= \\text{Attention} \\left( QW^Q_i, K W^K_i, VW^v_i \\right)\n",
    "\\end{align}$$\n",
    "\n",
    "__Query, Key and Value projection:__\n",
    "\n",
    "![picture](http://jalammar.github.io/images/t/self-attention-matrix-calculation.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07AkqQcqEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim,\n",
    "        num_heads,\n",
    "        dropout=0.0,\n",
    "        bias=False,\n",
    "        encoder_decoder_attention=False,  # otherwise self_attention\n",
    "        causal = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = emb_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.emb_dim, \"emb_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.encoder_decoder_attention = encoder_decoder_attention\n",
    "        self.causal = causal\n",
    "        self.k_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        \"\"\"\n",
    "        To-Do : Reshape input\n",
    "          Args : batch_size X sequence_length X embedding dimension\n",
    "          Return : batch_size X # attention head X sequence_length X head dimension\n",
    "        \"\"\"\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "        # This is equivalent to\n",
    "        # return x.transpose(1,2)\n",
    "    \n",
    "    def scaled_dot_product(self, \n",
    "                           query: torch.Tensor, \n",
    "                           key: torch.Tensor, \n",
    "                           value: torch.Tensor,\n",
    "                           attention_mask: torch.BoolTensor):\n",
    "        \"\"\"\n",
    "        To-Do : Implement scaled dot product\n",
    "          Args:\n",
    "            Query (Tensor): shape `(batch, seq_len, emb_dim)`\n",
    "            Key (Tensor): shape `(batch, seq_len, emb_dim)`\n",
    "            Value (Tensor): shape `(batch, seq_len, emb_dim)`\n",
    "            attention_mask: binary BoolTensor of shape `(batch, seq_len)` or `(seq_len, seq_len)`\n",
    "\n",
    "          Returns:\n",
    "            attn_output : attended output (result of attention mechanism)\n",
    "            attn_weights: value of each attention\n",
    "        \"\"\"\n",
    "        return attn_output, attn_weights\n",
    "    \n",
    "    def MultiHead_scaled_dot_product(self, \n",
    "                       query: torch.Tensor, \n",
    "                       key: torch.Tensor, \n",
    "                       value: torch.Tensor,\n",
    "                       attention_mask: torch.BoolTensor):\n",
    "        \"\"\"\n",
    "        To-Do : Implement Multi-head version of scaled dot product, please also take the causal masking into account.\n",
    "          Args:\n",
    "            Query (Tensor): shape `(batch,# attention head, seq_len, head_dim)`\n",
    "            Key (Tensor): shape `(batch,# attention head, seq_len, head_dim)`\n",
    "            Value (Tensor): shape `(batch,# attention head, seq_len, head_dim)`\n",
    "            attention_mask: binary BoolTensor of shape `(batch, src_len)` or `(seq_len, seq_len)`\n",
    "\n",
    "          Returns:\n",
    "            attn_output : attended output (result of attention mechanism)\n",
    "            attn_weights: value of each attention\n",
    "        \"\"\"\n",
    "\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        ):\n",
    "        q = self.q_proj(query)\n",
    "        # Enc-Dec attention\n",
    "        if self.encoder_decoder_attention:\n",
    "            k = self.k_proj(key)\n",
    "            v = self.v_proj(key)\n",
    "        # Self attention\n",
    "        else:\n",
    "            k = self.k_proj(query)\n",
    "            v = self.v_proj(query)\n",
    "\n",
    "        q = self.transpose_for_scores(q)\n",
    "        k = self.transpose_for_scores(k)\n",
    "        v = self.transpose_for_scores(v)\n",
    "\n",
    "        attn_output, attn_weights = self.MultiHead_scaled_dot_product(q,k,v,attention_mask)\n",
    "        return attn_output, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b528gtwHEbN8"
   },
   "source": [
    "### b. Position-wise feed forward network\n",
    "<a id='1b'></a>\n",
    "In this section, we will implement position-wise feed forward network\n",
    "\n",
    "$$\\text{FFN}(x) = \\max \\left(0, x W_1 + b_1 \\right) W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBqWWdIyEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim: int, d_ff: int, dropout: float = 0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.w_1 = nn.Linear(emb_dim, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, emb_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        To-Do : Implement position-wise feed forward network\n",
    "          Args:\n",
    "            x (Tensor): input to the layer of shape `(batch, seq_len, emb_dim)`\n",
    "        \"\"\"\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9-qkoUKEbN8"
   },
   "source": [
    "### c. Sinusoidal Positional Encoding\n",
    "<a id='1c'></a>\n",
    "In this section, we will implement sinusoidal positional encoding\n",
    "\n",
    "$$\\begin{align}\n",
    "PE(pos, 2i) &= \\sin \\left( pos / 10000^{2i / d_{model}} \\right)  \\\\\n",
    "PE(pos, 2i+1) &= \\cos \\left( pos / 10000^{2i / d_{model}} \\right)  \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsiJalEvEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SinusoidalPositionalEmbedding(nn.Embedding):\n",
    "    def __init__(self, num_positions, embedding_dim, padding_idx=None):\n",
    "        super().__init__(num_positions, embedding_dim)\n",
    "        self.weight = self._init_weight(self.weight)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _init_weight(out: nn.Parameter):\n",
    "        n_pos, embed_dim = out.shape\n",
    "        pe = nn.Parameter(torch.zeros(out.shape))\n",
    "        for pos in range(n_pos):\n",
    "            for i in range(0, embed_dim, 2):\n",
    "              \"\"\"\n",
    "              To-Do : Implement sinusoidal positional encoding\n",
    "              \"\"\"\n",
    "        pe.detach_()\n",
    "                \n",
    "        return pe\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, input_ids):\n",
    "        bsz, seq_len = input_ids.shape[:2]\n",
    "        positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\n",
    "        return super().forward(positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdhwI3hPEbN8"
   },
   "source": [
    "## 2. Transformer Encoder\n",
    "\n",
    "Now we have all basic building blocks which are essential to build Transformer. \n",
    "\n",
    "Let's implement Transformer step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6ym2hKzEbN8"
   },
   "source": [
    "### a. Encoder layer\n",
    "In this section, we will implement single layer of Transformer encoder.\n",
    "![picture](https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6B93kjUlEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.emb_dim = config.emb_dim\n",
    "        self.ffn_dim = config.ffn_dim\n",
    "        self.self_attn = MultiHeadAttention(            \n",
    "            emb_dim=self.emb_dim,\n",
    "            num_heads=config.attention_heads, \n",
    "            dropout=config.attention_dropout)\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
    "        self.dropout = config.dropout\n",
    "        self.activation_fn = nn.ReLU()\n",
    "        self.PositionWiseFeedForward = PositionWiseFeedForward(self.emb_dim, self.ffn_dim, config.dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.emb_dim)\n",
    "\n",
    "    def forward(self, x, encoder_padding_mask):\n",
    "        \"\"\"\n",
    "        To-Do : Implement transformer encoder layer\n",
    "          Args:\n",
    "            x (Tensor): input to the layer of shape `(batch, seq_len, emb_dim)`\n",
    "            encoder_padding_mask: binary BoolTensor of shape `(batch, src_len)`\n",
    "\n",
    "          Returns:\n",
    "            x : encoded output of shape `(batch, seq_len, emb_dim)`\n",
    "            self_attn_weights: self attention score\n",
    "        \"\"\"\n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LygNGzM0EbN8"
   },
   "source": [
    "### b. Encoder\n",
    "\n",
    "Stack encoder layers and build full Transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZOAlAv7EbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, embed_tokens):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        emb_dim = embed_tokens.embedding_dim\n",
    "        self.padding_idx = embed_tokens.padding_idx\n",
    "        self.max_source_positions = config.max_position_embeddings\n",
    "\n",
    "        self.embed_tokens = embed_tokens\n",
    "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
    "                config.max_position_embeddings, config.emb_dim, self.padding_idx\n",
    "            )\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        To-Do : Implement the transformer encoder\n",
    "          Args:\n",
    "            input_ids (Tensor): input to the layer of shape `(batch, seq_len)`\n",
    "            attention_mask: binary BoolTensor of shape `(batch, src_len)`\n",
    "\n",
    "          Returns:\n",
    "            x: encoded output of shape `(batch, seq_len, emb_dim)`\n",
    "            self_attn_scores: a list of self attention score of each layer\n",
    "        \"\"\"\n",
    "\n",
    "        return x, self_attn_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgjqDJnKEbN8"
   },
   "source": [
    "## 3. Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73LEB0mBEbN8"
   },
   "source": [
    "### a.Decoder layer\n",
    "In this section, we will implement single layer of Transformer decoder.\n",
    "![picture](http://incredible.ai/assets/images/transformer-decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HgMu2QCEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.emb_dim = config.emb_dim\n",
    "        self.ffn_dim = config.ffn_dim\n",
    "        self.self_attn = MultiHeadAttention(\n",
    "            emb_dim=self.emb_dim,\n",
    "            num_heads=config.attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            causal=True,\n",
    "        )\n",
    "        self.dropout = config.dropout\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
    "        self.encoder_attn = MultiHeadAttention(\n",
    "            emb_dim=self.emb_dim,\n",
    "            num_heads=config.attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            encoder_decoder_attention=True,\n",
    "        )\n",
    "        self.encoder_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
    "        self.PositionWiseFeedForward = PositionWiseFeedForward(self.emb_dim, self.ffn_dim, config.dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.emb_dim)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        encoder_hidden_states,\n",
    "        encoder_attention_mask=None,\n",
    "        causal_mask=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        To-Do : Implement the transformer decoder layer\n",
    "          Args:\n",
    "            x (Tensor): input to the layer of shape `(batch, seq_len, emb_dim)`\n",
    "            encoder_hidden_states: output from the encoder, used for\n",
    "                encoder-side attention\n",
    "            encoder_attention_mask: binary BoolTensor of shape `(batch, src_len)` to mask out encoder padding\n",
    "            causal_mask: binary BoolTensor of shape `(batch, src_len)` to mask out future tokens in decoder.\n",
    "\n",
    "\n",
    "          Returns:\n",
    "            x: decoded output of shape `(batch, seq_len, emb_dim)`\n",
    "            self_attn_weights: self attention score\n",
    "            cross_attn_weights: encoder-decoder attention score\n",
    "        \"\"\"\n",
    "        return (\n",
    "            x,\n",
    "            self_attn_weights,\n",
    "            cross_attn_weights,\n",
    "        ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAJQ-q5mEbN8"
   },
   "source": [
    "### b. Decoder\n",
    "\n",
    "Stack decoder layers and build full Transformer decoder.\n",
    "\n",
    "Unlike the encoder, you need to do one more job: pass the causal(unidirectional) mask to the decoder self attention layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEMa6owhEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a :class:`DecoderLayer`\n",
    "\n",
    "    Args:\n",
    "        config: BartConfig\n",
    "        embed_tokens (torch.nn.Embedding): output embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, embed_tokens: nn.Embedding):\n",
    "        super().__init__()\n",
    "        self.dropout = config.dropout\n",
    "        self.padding_idx = embed_tokens.padding_idx\n",
    "        self.max_target_positions = config.max_position_embeddings\n",
    "        self.embed_tokens = embed_tokens\n",
    "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
    "            config.max_position_embeddings, config.emb_dim, self.padding_idx\n",
    "        )\n",
    "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.decoder_layers)])  # type: List[DecoderLayer]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        encoder_hidden_states,\n",
    "        encoder_attention_mask,\n",
    "        decoder_causal_mask,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        To-Do : Implement the transformer decoder\n",
    "\n",
    "        Args:\n",
    "            input_ids (LongTensor): previous decoder outputs of shape\n",
    "                `(batch, tgt_len)`, for teacher forcing\n",
    "            encoder_hidden_states: output from the encoder, used for\n",
    "                encoder-side attention\n",
    "            encoder_attention_mask: binary BoolTensor of shape `(batch, src_len)` to mask out encoder padding\n",
    "            causal_mask: binary BoolTensor of shape `(batch, src_len)` to mask out future tokens in decoder.\n",
    "\n",
    "          Returns:\n",
    "            x: decoded output of shape `(batch, seq_len, emb_dim)`\n",
    "            cross_attn_scores: list of encoder-decoder attention score of each layer\n",
    "        \"\"\"\n",
    "\n",
    "        return x, cross_attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr0g3oeIEbN8"
   },
   "source": [
    "## 4. Transformer\n",
    "\n",
    "Let's combine encoder and decoder in one place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4aZzq8GEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, SRC_vocab, TRG_vocab, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.SRC_vocab = SRC_vocab\n",
    "        self.TRG_vocab = TRG_vocab\n",
    "\n",
    "        self.enc_embedding = nn.Embedding(len(SRC_vocab.itos), config.emb_dim, padding_idx=SRC_vocab.stoi['<pad>'])\n",
    "        self.dec_embedding = nn.Embedding(len(TRG_vocab.itos), config.emb_dim, padding_idx=TRG_vocab.stoi['<pad>'])\n",
    "\n",
    "        self.encoder = Encoder(config, self.enc_embedding)\n",
    "        self.decoder = Decoder(config, self.dec_embedding)\n",
    "\n",
    "        self.prediction_head = nn.Linear(config.emb_dim, len(TRG_vocab.itos))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_mask(self, src, trg):\n",
    "        '''\n",
    "        To-Do : Generate mask for encoder and decoder attention.\n",
    "\n",
    "        Args:\n",
    "            src(LongTensor): Input to the transformer of shape (batch_size, seq_len)  \n",
    "            trg(LongTensor): Decoding target of the transformer of shape (batch_size, seq_len)  \n",
    "\n",
    "            Returns:\n",
    "            enc_attention_mask: padding mask for encoder\n",
    "            dec_attention_mask: causal mask for decoder\n",
    "        '''\n",
    "\n",
    "        return enc_attention_mask, dec_attention_mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "                else:\n",
    "                    nn.init.constant_(param.data, 0)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        enc_attention_mask, dec_causal_mask = self.generate_mask(src, trg)\n",
    "        encoder_output, encoder_attention_scores = self.encoder(\n",
    "            input_ids=src,\n",
    "            attention_mask=enc_attention_mask\n",
    "        )\n",
    "\n",
    "        decoder_output, decoder_attention_scores = self.decoder(\n",
    "            trg,\n",
    "            encoder_output,\n",
    "            encoder_attention_mask=enc_attention_mask,\n",
    "            decoder_causal_mask=dec_causal_mask,\n",
    "        )\n",
    "        decoder_output = self.prediction_head(decoder_output)\n",
    "\n",
    "        return decoder_output, encoder_attention_scores, decoder_attention_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WU-llE39EbN8"
   },
   "source": [
    "# III. Train & Evaluate\n",
    "<a id='3'></a>\n",
    "This section is very similar to week 9, so please refer to it for detailed description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZRMlUmxEbN8"
   },
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlIc_VKaEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import easydict\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "config = easydict.EasyDict({\n",
    "    \"emb_dim\": 64,\n",
    "    \"ffn_dim\": 256,\n",
    "    \"attention_heads\": 4,\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"dropout\": 0.2,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"encoder_layers\": 3,\n",
    "    \"decoder_layers\": 3,\n",
    "})\n",
    "\n",
    "N_EPOCHS = 100\n",
    "learning_rate = 5e-4\n",
    "CLIP = 1\n",
    "\n",
    "PAD_IDX = SRC_vocab.stoi['<pad>']\n",
    "\n",
    "model = Transformer(SRC_vocab, TRG_vocab, config)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hql5wOKEbN8"
   },
   "source": [
    "## 2. Train & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1HHCxXuEbN8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: DataLoader,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        src = batch['SRC'].to(device)\n",
    "        trg = batch['TRG'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, enc_attention_scores, _ = model(src, trg)\n",
    "\n",
    "        output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
    "        trg = trg[:,1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: DataLoader,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            src = batch['SRC'].to(device)\n",
    "            trg = batch['TRG'].to(device)\n",
    "\n",
    "\n",
    "            output, attention_score, _ = model(src, trg)\n",
    "\n",
    "            output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
    "            trg = trg[:,1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS), total=N_EPOCHS):\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    else: \n",
    "        break\n",
    "\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxyJad1WEbN8"
   },
   "source": [
    "# IV. Visualization\n",
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_Eop7pGEbN8"
   },
   "source": [
    "## 1. Positional embedding visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJKGr5JfEbN8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(15, 9))\n",
    "cax = ax.matshow(model.encoder.embed_positions.weight.data.cpu().numpy(), aspect='auto',cmap=plt.cm.YlOrRd)\n",
    "fig.colorbar(cax)\n",
    "ax.set_title('Positional Embedding Matrix', fontsize=18)\n",
    "ax.set_xlabel('Embedding Dimension', fontsize=14)\n",
    "ax.set_ylabel('Sequence Length', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrLCVOWlEbN8"
   },
   "source": [
    "## 2. Attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azwmQfF-EbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from attentionviz import head_view\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLCz7R73EbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if not 'attentionviz' in sys.path:\n",
    "  sys.path += ['attentionviz']\n",
    "!pip install regex\n",
    "\n",
    "def call_html():\n",
    "  import IPython\n",
    "  display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
    "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkV8XEM2EbN9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SAMPLE_IDX = 131\n",
    "\n",
    "sample = test_data[SAMPLE_IDX]\n",
    "\n",
    "src_numericalized = torch.LongTensor([SRC_vocab.numericalize(sample['SRC'])]).to(device)\n",
    "trg_numericalized = torch.LongTensor([TRG_vocab.numericalize(sample['TRG'])]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, enc_attention_score, dec_attention_score = model(src_numericalized, trg_numericalized) # turn off teacher forcing\n",
    "    attention_score = {'self': enc_attention_score, 'cross': dec_attention_score}\n",
    "\n",
    "src_tok = [SRC_vocab.itos[x] for x in src_numericalized.squeeze()]\n",
    "trg_tok = [TRG_vocab.itos[x] for x in trg_numericalized.squeeze()]\n",
    "\n",
    "call_html()\n",
    "head_view(attention_score, src_tok, trg_tok)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ai504_10_transformer.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
